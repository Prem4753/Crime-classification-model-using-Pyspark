{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                     __Crime Classification Model using Pyspark__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __1. Setup Spark and load other libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"clipper-pyspark\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "np.random.seed(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __2. Data Extraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates,Category,Descript,DayOfWeek,PdDistrict,Resolution,Address,X,Y\n",
      "2015-05-13 23:53:00,WARRANTS,WARRANT ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747\n",
      "2015-05-13 23:53:00,OTHER OFFENSES,TRAFFIC VIOLATION ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",OAK ST / LAGUNA ST,-122.425891675136,37.7745985956747\n",
      "2015-05-13 23:33:00,OTHER OFFENSES,TRAFFIC VIOLATION ARREST,Wednesday,NORTHERN,\"ARREST, BOOKED\",VANNESS AV / GREENWICH ST,-122.42436302145,37.8004143219856\n",
      "2015-05-13 23:30:00,LARCENY/THEFT,GRAND THEFT FROM LOCKED AUTO,Wednesday,NORTHERN,NONE,1500 Block of LOMBARD ST,-122.42699532676599,37.80087263276921\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#Let see the first 5 rows\n",
    "head -5 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Structure\n",
      "----------------------------------\n",
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n",
      "None\n",
      " \n",
      "Dataframe preview\n",
      "+--------------+--------------------+\n",
      "|      Category|         Description|\n",
      "+--------------+--------------------+\n",
      "|      warrants|      warrant arrest|\n",
      "|other offenses|traffic violation...|\n",
      "|other offenses|traffic violation...|\n",
      "| larceny/theft|grand theft from ...|\n",
      "| larceny/theft|grand theft from ...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      " \n",
      "----------------------------------\n",
      "Total number of rows 878049\n"
     ]
    }
   ],
   "source": [
    "#Read the data into spark datafrome\n",
    "from pyspark.sql.functions import col, lower\n",
    "df = spark.read.format('csv')\\\n",
    "          .option('header','true')\\\n",
    "          .option('inferSchema', 'true')\\\n",
    "          .option('timestamp', 'true')\\\n",
    "          .load('train.csv')\n",
    "\n",
    "data = df.select(lower(col('Category')),lower(col('Descript')))\\\n",
    "        .withColumnRenamed('lower(Category)','Category')\\\n",
    "        .withColumnRenamed('lower(Descript)', 'Description')\n",
    "data.cache()\n",
    "print('Dataframe Structure')\n",
    "print('----------------------------------')\n",
    "print(data.printSchema())\n",
    "print(' ')\n",
    "print('Dataframe preview')\n",
    "print(data.show(5))\n",
    "print(' ')\n",
    "print('----------------------------------')\n",
    "print('Total number of rows', df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: __To familiar ourselves with the dataset, we need to see the top list of the crime categories and descriptions__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Category: 39\n",
      " \n",
      "Top 10 Crime Category\n",
      "+--------------+----------+\n",
      "|      Category|totalValue|\n",
      "+--------------+----------+\n",
      "| larceny/theft|    174900|\n",
      "|other offenses|    126182|\n",
      "|  non-criminal|     92304|\n",
      "|       assault|     76876|\n",
      "| drug/narcotic|     53971|\n",
      "| vehicle theft|     53781|\n",
      "|     vandalism|     44725|\n",
      "|      warrants|     42214|\n",
      "|      burglary|     36755|\n",
      "|suspicious occ|     31414|\n",
      "+--------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      " \n",
      " \n",
      "Total number of unique value of Description: 879\n",
      " \n",
      "Top 10 Crime Description\n",
      "+--------------------+----------+\n",
      "|         Description|totalValue|\n",
      "+--------------------+----------+\n",
      "|grand theft from ...|     60022|\n",
      "|       lost property|     31729|\n",
      "|             battery|     27441|\n",
      "|   stolen automobile|     26897|\n",
      "|drivers license, ...|     26839|\n",
      "|      warrant arrest|     23754|\n",
      "|suspicious occurr...|     21891|\n",
      "|aided case, menta...|     21497|\n",
      "|petty theft from ...|     19771|\n",
      "|malicious mischie...|     17789|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_n_list(df,var, N):\n",
    "    '''\n",
    "    This function determine the top N numbers of the list\n",
    "    '''\n",
    "    print(\"Total number of unique value of\"+' '+var+''+':'+' '+str(df.select(var).distinct().count()))\n",
    "    print(' ')\n",
    "    print('Top'+' '+str(N)+' '+'Crime'+' '+var)\n",
    "    df.groupBy(var).count().withColumnRenamed('count','totalValue')\\\n",
    "    .orderBy(col('totalValue').desc()).show(N)\n",
    "    \n",
    "    \n",
    "top_n_list(data, 'Category',10)\n",
    "print(' ')\n",
    "print(' ')\n",
    "top_n_list(data,'Description',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: __Category feature will be our label (multi-class). How many classes?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('Category').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __3. Partition the dataset into Training and Test dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 615417\n",
      "Test Dataset Count: 262632\n"
     ]
    }
   ],
   "source": [
    "training, test = data.randomSplit([0.7,0.3], seed=60)\n",
    "#trainingSet.cache()\n",
    "print(\"Training Dataset Count:\", training.count())\n",
    "print(\"Test Dataset Count:\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __4. Define Structure to build Pipeline__\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes \n",
    "\n",
    "#----------------Define tokenizer with regextokenizer()------------------\n",
    "regex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n",
    "                  .setInputCol(\"Description\")\\\n",
    "                  .setOutputCol(\"tokens\")\n",
    "\n",
    "#----------------Define stopwords with stopwordsremover()---------------------\n",
    "extra_stopwords = ['http','amp','rt','t','c','the']\n",
    "stopwords_remover = StopWordsRemover()\\\n",
    "                    .setInputCol('tokens')\\\n",
    "                    .setOutputCol('filtered_words')\\\n",
    "                    .setStopWords(extra_stopwords)\n",
    "                    \n",
    "\n",
    "#----------Define bags of words using countVectorizer()---------------------------\n",
    "count_vectors = CountVectorizer(vocabSize=10000, minDF=5)\\\n",
    "               .setInputCol(\"filtered_words\")\\\n",
    "               .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "#-----------Using TF-IDF to vectorise features instead of countVectoriser-----------------\n",
    "hashingTf = HashingTF(numFeatures=10000)\\\n",
    "            .setInputCol(\"filtered_words\")\\\n",
    "            .setOutputCol(\"raw_features\")\n",
    "            \n",
    "#Use minDocFreq to remove sparse terms\n",
    "idf = IDF(minDocFreq=5)\\\n",
    "        .setInputCol(\"raw_features\")\\\n",
    "        .setOutputCol(\"features\")\n",
    "\n",
    "#---------------Define bag of words using Word2Vec---------------------------\n",
    "word2Vec = Word2Vec(vectorSize=1000, minCount=0)\\\n",
    "           .setInputCol(\"filtered_words\")\\\n",
    "           .setOutputCol(\"features\")\n",
    "\n",
    "#-----------Encode the Category variable into label using StringIndexer-----------\n",
    "label_string_idx = StringIndexer()\\\n",
    "                  .setInputCol(\"Category\")\\\n",
    "                  .setOutputCol(\"label\")\n",
    "\n",
    "#-----------Define classifier structure for logistic Regression--------------\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "#---------Define classifier structure for Naive Bayes----------\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "\n",
    "def metrics_ev(labels, metrics):\n",
    "    '''\n",
    "    List of all performance metrics\n",
    "    '''\n",
    "    # Confusion matrix\n",
    "    print(\"---------Confusion matrix-----------------\")\n",
    "    print(metrics.confusionMatrix)\n",
    "    print(' ')    \n",
    "    # Overall statistics\n",
    "    print('----------Overall statistics-----------')\n",
    "    print(\"Precision = %s\" %  metrics.precision())\n",
    "    print(\"Recall = %s\" %  metrics.recall())\n",
    "    print(\"F1 Score = %s\" % metrics.fMeasure())\n",
    "    print(' ')\n",
    "    # Statistics by class\n",
    "    print('----------Statistics by class----------')\n",
    "    for label in sorted(labels):\n",
    "       print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "       print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "       print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "    print(' ')\n",
    "    # Weighted stats\n",
    "    print('----------Weighted stats----------------')\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __6. Build Multi-Classification__\n",
    "__The stages involve to perform multi-classification include:__\n",
    "1. Model training and evaluation\n",
    "   1. Build baseling model\n",
    "      1. Logistic regression using CountVectorizer features\n",
    "   2. Build secondary models\n",
    "      1. Naive Bayes\n",
    "      2. Logistic regression and Naive Bayes using TF-IDF features\n",
    "    \n",
    " ### __(i) Baseline Model__ \n",
    "\n",
    "\n",
    "#### __(a). Apply Logistic Regression with  Count Vector Features__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, lr])\n",
    "model_cv_lr = pipeline_cv_lr.fit(training)\n",
    "predictions_cv_lr = model_cv_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n",
      " \n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                   Description|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8726782249097988,0.02162...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8726782249097988,0.02162...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8726782249097988,0.02162...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8726782249097988,0.02162...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8726782249097988,0.02162...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "predictions_cv_lr.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------Accuracy----------------------------------\n",
      " \n",
      "                       accuracy:0.9721844116763713:\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "evaluator_cv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_lr)\n",
    "print(' ')\n",
    "print('------------------------------Accuracy----------------------------------')\n",
    "print(' ')\n",
    "print('                       accuracy:{}:'.format(evaluator_cv_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### __(ii). Secondary Models__\n",
    " #### __(a). Apply Naive Bayes with Count Vector Features__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Secondary model using NaiveBayes\n",
    "pipeline_cv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, nb])\n",
    "model_cv_nb = pipeline_cv_nb.fit(training)\n",
    "predictions_cv_nb = model_cv_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------Accuracy-----------------------------\n",
      " \n",
      "                      accuracy:0.9933012222188159:\n"
     ]
    }
   ],
   "source": [
    "evaluator_cv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_nb)\n",
    "print(' ')\n",
    "print('--------------------------Accuracy-----------------------------')\n",
    "print(' ')\n",
    "print('                      accuracy:{}:'.format(evaluator_cv_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __(b). Apply Logistic Regression Using TF-IDF Features__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_idf_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, lr])\n",
    "model_idf_lr = pipeline_idf_lr.fit(training)\n",
    "predictions_idf_lr = model_idf_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Check Top 5 predictions----------------------------------\n",
      " \n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                   Description|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8745035002793186,0.02115...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8745035002793186,0.02115...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8745035002793186,0.02115...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8745035002793186,0.02115...|  0.0|       0.0|\n",
      "|theft, bicycle, <$50, no se...|larceny/theft|[0.8745035002793186,0.02115...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------Check Top 5 predictions----------------------------------')\n",
    "print(' ')\n",
    "predictions_idf_lr.select('Description','Category',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "-------------------------------Accuracy---------------------------------\n",
      " \n",
      "                        accuracy:0.9723359770202158:\n"
     ]
    }
   ],
   "source": [
    "evaluator_idf_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_lr)\n",
    "print(' ')\n",
    "print('-------------------------------Accuracy---------------------------------')\n",
    "print(' ')\n",
    "print('                        accuracy:{}:'.format(evaluator_idf_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __(c). Apply Naive Bayes with TF-IDF Features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_idf_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, nb])\n",
    "model_idf_nb = pipeline_idf_nb.fit(training)\n",
    "predictions_idf_nb = model_idf_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "-----------------------------Accuracy-----------------------------\n",
      " \n",
      "                          accuracy:0.9950758205262961:\n"
     ]
    }
   ],
   "source": [
    "evaluator_idf_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_nb)\n",
    "print(' ')\n",
    "print('-----------------------------Accuracy-----------------------------')\n",
    "print(' ')\n",
    "print('                          accuracy:{}:'.format(evaluator_idf_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. __Results:__\n",
    "__The table below has accuracy of the models generated by different extraction techniques.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    | Logistic Regression | Naive Bayes |\n",
    "| -------------------|:-------------------:|------------:|\n",
    "| Count Vectoriser   |  97.2%              |   99.3%     |\n",
    "| TF-IDF             |  97.2%              |   99.5%     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: __As you can see, TF-IDF proves to be best vectoriser for this dataset, while Naive Bayes proves to be better algorithm for text analysis than Logistic regression.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __7. Deploy the Model__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "MODEL=pyspark.ml.PipelineModel(\"spark-naive-bayes-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTP_BAD_REQUEST = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict')\n",
    "def predict():\n",
    "    Description = request.args.get('Description', default=None, type=str)\n",
    "    \n",
    "    # Reject request that have bad or missing values.\n",
    "    if Description is None:\n",
    "        # Provide the caller with feedback on why the record is unscorable.\n",
    "        message = ('Record cannot be scored because of '\n",
    "                   'missing or unacceptable values. '\n",
    "                   'All values must be present and of type string.')\n",
    "        response = jsonify(status='error',\n",
    "                           error_message=message)\n",
    "        # Sets the status code to 400\n",
    "        response.status_code = HTTP_BAD_REQUEST\n",
    "        return response\n",
    "    \n",
    "    features = [[Description]]\n",
    "    predictions = MODEL.transform(features)\n",
    "    label_pred = predictions.select(\"Description\",\"Category\",\"probability\",\"prediction\")\n",
    "    return jsonify(status='complete', label=label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "nteract-on-jupyter@1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
